{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The rapid growth of Next Generation Sequencing technologies such as single-cell RNA sequencing(scRNA-seq) demands efficient parallel processing and analysis of big data. Hadoop and Spark are the goto opensource frameworks for storing and processing massive datasets. The most significant advantage of Spark is its iterative analytics capability combined with in-memory computing architecture. Calling .cache() on a resilient distributed dataset (RDD) effectively saves it in memory and makes it instantly available for computation, thus the subsequent filter, map, and reduce tasks become instantaneous. Spark has its query language known as Spark SQL, and its MLlib library is highly desirable for machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create virtual environment\n",
    "\n",
    "There are planty of ways to create python's virtual environment, but the easies is to run `venv` buildin module.\n",
    "\n",
    "```sh\n",
    "python3 -m venv .venv\n",
    "```\n",
    "\n",
    "Once the environment is created you can activate it by running the following command:\n",
    "\n",
    "```sh\n",
    "source .venv/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing pyspark\n",
    "\n",
    "To install pyspark you run command:\n",
    "\n",
    "```sh\n",
    "pip install pyspark\n",
    "```\n",
    "\n",
    "Then to execute REPL just type `pyspark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring pyspark to use ipython as the command shel in the REPL\n",
    "\n",
    "But default REPL is not that good, and IPython is must better (TBD: expand on this).\n",
    "\n",
    "```sh\n",
    "pip install ipython\n",
    "```\n",
    "\n",
    "Then to enable ipython interpreter in pyspark repl you will need to setup environment variable `PYSPARK_DRIVER_PYTHON`.\n",
    "\n",
    "To do so run the following command in the active environment:\n",
    "\n",
    "```sh\n",
    "export PYSPARK_DRIVER_PYTHON=ipython\n",
    "```\n",
    "\n",
    "In some environments this may not work and the command can tell that\n",
    "\n",
    "```sh\n",
    ".../bin/load-spark-env.sh: No such file or directory\n",
    "```\n",
    "\n",
    "to fix the issue set `SPARK_HOME` variable:\n",
    "\n",
    "```sh\n",
    "export SPARK_HOME=.../venv/lib/python3.7/site-packages/pyspark\n",
    "```\n",
    "\n",
    "Now if you run `pyspark` command it will use `ipython` command shell.\n",
    "\n",
    "And you will have code completion and code highlihting in the Repl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Jupyter notebook\n",
    "\n",
    "\n",
    "First of all you need to install Jupyter.\n",
    "\n",
    "```sh\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Then similar to `ipython` you need to set `PYSPARK_DRIVER_PYTHON` and `PYSPARK_DRIVER_PYTHON_OPTS` variables.\n",
    "\n",
    "```sh\n",
    "export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS=notebook\n",
    "```\n",
    "\n",
    "To ensure that integration with Jupyter notebook works as expected, run the `pyspark` command and in the code cell of the notebook printing `sc` variable. This is default Spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.49:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use jupyterlab\n",
    "\n",
    "There is a jupyterlab project, which will replace notebooks. To use it just install it.\n",
    "\n",
    "```sh\n",
    "pip install jupyterlab\n",
    "```\n",
    "\n",
    "and set environment variables.\n",
    "\n",
    "```sh\n",
    "export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS=lab\n",
    "```\n",
    "\n",
    "### Spark context\n",
    "\n",
    "\n",
    "If you run a standalone script you supposed to create spark context yourself. Typically you do it by\n",
    "\n",
    "```python\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "\n",
    "conf = SparkConf() \\\n",
    "  .setAppName(\"appName\") \\\n",
    "  .setMaster(\"local\") \\\n",
    "  .set(\"spark.sample.config\", \"value\")\n",
    "sc = SparkContext(conf=conf)\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(master=\"local\", appName=\"test_name\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Instead of having a spark context, hive context, SQL context, starting from Spark 2.0 all of it is encapsulated in a Spark session. You can create spark session yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.49:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1121853d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc2 = SparkSession.builder \\\n",
    "  .master(\"local[*]\") \\\n",
    "  .appName(\"name of the script\") \\\n",
    "  .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "  .getOrCreate()\n",
    "sc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also you can craete a new SparkSession from existing SparkContext by passing it into the initializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.49:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x112197bd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc3 = SparkSession(sc)\n",
    "sc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, we don't have to create a spark session object when using spark-shell. It is already created for us with the variable `spark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.49:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1121853d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see https://spark.apache.org/docs/2.4.4/api/python/pyspark.sql.html?highlight=sparksession for interface of the builder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the example we have set `spark.some.config.option` setting to `some-value`. There lots of such properties which that can control important aspects of Spark.\n",
    "Consider for example `spark.submit.pyFiles` or `spark.jars` properties. \n",
    "\n",
    "You can find complete list of all avaiable settings here https://spark.apache.org/docs/2.4.4/configuration.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using google colab\n",
    "\n",
    "Though it is not that difficult to setup development environment on you local computer, it is event easier to use google colab notebooks. You do not need to have a powerfull computer...\n",
    "\n",
    "But before you can install pyspark on the colab you will have to install additional compotents:\n",
    "\n",
    "In the first colab cell type:\n",
    "\n",
    "```sh\n",
    "# !apt-get --quiet install -y openjdk-8-jdk-headless\n",
    "# !update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
    "!pip install pyspark\n",
    "```\n",
    "\n",
    "Nowadays it is enough to just install `pyspark` because `java` is already installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some setting of the spark\n",
    "\n",
    "```\n",
    ".set('spark.executor.memory', '4G')\n",
    ".set('spark.driver.memory', '16G')\n",
    ".set('spark.driver.maxResultSize', '8G')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download FASTQ file\n",
    "\n",
    "```sh\n",
    "!mkdir /content\n",
    "!mkdir /content/data\n",
    "!cd /content\n",
    "\n",
    "!wget 'https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.9.6-1/sratoolkit.2.9.6-1-ubuntu64.tar.gz'\n",
    "!gunzip sratoolkit.2.9.6-1-ubuntu64.tar.gz\n",
    "!tar -xf sratoolkit.2.9.6-1-ubuntu64.tar\n",
    "\n",
    "!wget https://sra-download.ncbi.nlm.nih.gov/traces/era6/ERR/ERR3014/ERR3014700\n",
    "!/content/sratoolkit.2.9.6-1-ubuntu64/bin/fastq-dump /content/ERR3014700 -O /content/data\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
